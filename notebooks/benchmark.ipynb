{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chainlit.utils import mount_chainlit\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, Iterable, List, Optional\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from pydantic import BaseModel\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from enum import Enum\n",
    "\n",
    "logging.basicConfig(filename=\"query_logs.log\", level=logging.INFO)\n",
    "\n",
    "class BertHFPath(str, Enum):\n",
    "    modern_bert_large_embed = \"lightonai/modernbert-embed-large\"\n",
    "    modern_bert_base_embed = \"nomic-ai/modernbert-embed-base\"\n",
    "    modern_bert_base = \"answerdotai/ModernBERT-base\"\n",
    "    modern_bert_large = \"answerdotai/ModernBERT-large\"\n",
    "    gte_base = \"Alibaba-NLP/gte-base-en-v1.5\"\n",
    "    gte_large = \"Alibaba-NLP/gte-large-en-v1.5\"\n",
    "    modern_bert_base_v2 = \"Alibaba-NLP/gte-modernbert-base\"\n",
    "    daberta_base = \"microsoft/deberta-v3-base\"\n",
    "\n",
    "class BenchmarkQueryRequest(BaseModel):\n",
    "    encoder_path:BertHFPath =  BertHFPath.modern_bert_base\n",
    "    batch_size: int = 20\n",
    "    num_pass: Optional[int] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterator, List, Optional, cast\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n",
    "class DatasetWrapper:\n",
    "    def __init__(\n",
    "        self,\n",
    "        path_to_files: str,\n",
    "    ) -> None:\n",
    "        self.root = path_to_files\n",
    "        self.find_all_files()\n",
    "\n",
    "    def find_all_files(self) -> None:\n",
    "        self.list_files: List[Path] = list(Path(self.root).rglob(\"*.txt\"))\n",
    "        assert len(self.list_files), \"No documents found\"\n",
    "\n",
    "    def _read_file_in_chunks(\n",
    "        self,\n",
    "        file_path: Path,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        chunk_size: int,\n",
    "        use_random_chunk_size: bool = False,\n",
    "    ) -> Iterator[Dict[str, str]]:\n",
    "        with file_path.open(encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "\n",
    "        tokens = tokenizer(content, truncation=False, return_tensors=\"pt\")[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        assert chunk_size is not None\n",
    "        index = 0\n",
    "        while index < len(tokens):\n",
    "            chunk_size = chunk_size if not use_random_chunk_size else random.randint(1, chunk_size)\n",
    "            chunk_tokens = tokens[index : index + chunk_size]\n",
    "            chunk_text = tokenizer.decode(\n",
    "                chunk_tokens.squeeze(0).tolist(), skip_special_tokens=True\n",
    "            )\n",
    "            index += chunk_size\n",
    "\n",
    "            yield {\n",
    "                \"id\": f\"{str(file_path).replace('/', '_')}_{index}_{chunk_size}\",\n",
    "                \"content\": chunk_text,\n",
    "            }\n",
    "\n",
    "    def get_iterator(\n",
    "        self,\n",
    "        tokenizer: AutoTokenizer,\n",
    "        chunk_size: int = 2048,\n",
    "        use_random_chunk_size: bool = False,\n",
    "    ) -> Iterator[Dict[str, str]]:\n",
    "        for file_path in self.list_files:\n",
    "            yield from self._read_file_in_chunks(\n",
    "                file_path, tokenizer, chunk_size, use_random_chunk_size\n",
    "            )\n",
    "\n",
    "    def get_dataloader(\n",
    "        self,\n",
    "        batch_size: int = 10,\n",
    "        tokenizer_path: BertHFPath = BertHFPath.modern_bert_base_embed,\n",
    "        tokenize: bool = False,\n",
    "        chunk_size: int = 2048,\n",
    "        use_random_chunk_size: bool = False,\n",
    "    ) -> DataLoader[Dict[str, torch.Tensor]]:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path.value)\n",
    "\n",
    "        dataset = Dataset.from_generator(\n",
    "            partial(\n",
    "                self.get_iterator,\n",
    "                tokenizer=tokenizer,\n",
    "                chunk_size=chunk_size,\n",
    "                use_random_chunk_size=use_random_chunk_size,\n",
    "            )\n",
    "        )\n",
    "        if tokenize:\n",
    "            dataset = dataset.map(partial(self.tokenization, tokenizer=tokenizer), batched=True)\n",
    "            columns_of_interest = {\"input_ids\", \"attention_mask\", \"token_type_ids\"}.intersection(\n",
    "                dataset.column_names\n",
    "            )\n",
    "\n",
    "            dataset.set_format(\"pt\", columns=columns_of_interest, output_all_columns=False)\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, drop_last=True, pin_memory=True)\n",
    "        return dataloader\n",
    "\n",
    "    def tokenization(\n",
    "        self,\n",
    "        example: Dict[str, torch.Tensor],\n",
    "        tokenizer: BertHFPath = BertHFPath.modern_bert_base_embed,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        return cast(\n",
    "            Dict[str, torch.Tensor],\n",
    "            tokenizer(example[\"content\"], return_tensors=\"pt\", padding=True),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "corpus = DatasetWrapper(\n",
    "        \"corpus/\",\n",
    "    )\n",
    "dataloader = corpus.get_dataloader(\n",
    "        batch_size=1, tokenizer_path=BertHFPath.modern_bert_base, chunk_size=8192, tokenize=True\n",
    "    )\n",
    "sum([k['input_ids'].shape[2] for k in dataloader])\n",
    "# np.mean([k['input_ids'].shape[2] for k in dataloader])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = DatasetWrapper(\n",
    "        \"corpus/\",\n",
    "    )\n",
    "dataloader = corpus.get_dataloader(\n",
    "        batch_size=1, tokenizer_path=BertHFPath.gte_base, chunk_size=512, tokenize=True\n",
    "    )\n",
    "sum([k['input_ids'].shape[2] for k in dataloader])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import time\n",
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"filelock\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "# Configure logging to only write to a file\n",
    "logging.basicConfig(\n",
    "    filename=\"query_logs.log\",  # Log file path\n",
    "    level=logging.INFO,  # Set log level\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Log format\n",
    "    filemode=\"w\",  # Overwrite the file on each run (use \"a\" to append)\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "\n",
    "\n",
    "def benchmark_on_corpus(\n",
    "    encoder_path: BertHFPath,\n",
    "    batch_size: int = 10,\n",
    "    chunk_size: int = 4096,\n",
    "    num_pass: int = 1,\n",
    "    use_random_chunk_size: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    torch.cuda.empty_cache()\n",
    "    model = AutoModel.from_pretrained(encoder_path.value, trust_remote_code=True)\n",
    "    model.to(device, torch.float16)\n",
    "    # model = torch.compile(model)\n",
    "    \n",
    "    corpus = DatasetWrapper(\n",
    "        \"corpus/\",\n",
    "    )\n",
    "    dataloader = corpus.get_dataloader(\n",
    "        batch_size=batch_size, tokenizer_path=encoder_path, chunk_size=chunk_size, tokenize=True\n",
    "    )\n",
    "\n",
    "    evaluation_times = []\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "            for docs in dataloader:\n",
    "                for i in range(2):\n",
    "                    inputs = {\n",
    "                        k: v.to(device) for k, v in docs.items() if k not in [\"content\", \"id\"]\n",
    "                    }\n",
    "                    model(**inputs)\n",
    "                break\n",
    "\n",
    "    num_tokken = 0\n",
    "    token_counts = []\n",
    "    device_times = []\n",
    "    start_time = time.perf_counter()\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "            for i in tqdm(range(num_pass)):\n",
    "                for docs in tqdm(dataloader):\n",
    "                    num_tokken += inputs[\"input_ids\"].numel()\n",
    "                    token_counts.append(inputs[\"input_ids\"].numel())\n",
    "                    \n",
    "                    start_event = torch.cuda.Event(enable_timing=True)\n",
    "                    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "                    start_event.record()\n",
    "                    inputs = {\n",
    "                        k: v.to(device, non_blocking=True)  # Use non-blocking transfers\n",
    "                        for k, v in docs.items() if k not in [\"content\", \"id\"]\n",
    "                    }\n",
    "                    end_event.record()\n",
    "                    torch.cuda.synchronize()  # Ensure all events are completed\n",
    "                    device_times.append(start_event.elapsed_time(end_event) / 1000)  # Convert ms to sec\n",
    "\n",
    "                    # Model forward pass timing\n",
    "                    start_event.record()\n",
    "                    _ = model(**inputs)\n",
    "                    end_event.record()\n",
    "                    torch.cuda.synchronize()\n",
    "                    evaluation_times.append(start_event.elapsed_time(end_event) / 1000)\n",
    "\n",
    "    total_time = time.perf_counter() - start_time\n",
    "\n",
    "    mean_time = np.mean(evaluation_times)\n",
    "    var_time = np.var(evaluation_times)\n",
    "\n",
    "    tokens_per_sec = np.sum(np.array(token_counts)) / total_time\n",
    "\n",
    "    results = {\n",
    "        \"exp\": {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"chunk_size\": chunk_size,\n",
    "            \"encoder\": encoder_path.value,\n",
    "            \"use_random_chunk_size\": use_random_chunk_size,\n",
    "        },\n",
    "        \"total_time\": str(total_time),\n",
    "        \"mean_time\": str(sum(evaluation_times) / len(evaluation_times)),\n",
    "        \"num_tokken\": num_tokken,\n",
    "        \"var\": var_time,\n",
    "        \"mean_time/tokken\": mean_time / np.mean(token_counts), \n",
    "        \"total_time/tokken\": total_time / np.sum(token_counts),\n",
    "        \"token/mean_time\": np.mean(token_counts)/ mean_time ,\n",
    "        \"token/total_time\":  np.sum(token_counts)/ total_time ,\n",
    "        \"tokken/sec\": tokens_per_sec,\n",
    "        \"device_time\": np.mean(device_times)\n",
    "    }\n",
    "\n",
    "    logging.info(json.dumps(results))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_on_corpus(BertHFPath.modern_bert_base, batch_size=1600, chunk_size=512, num_pass=2, use_random_chunk_size=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_on_corpus(BertHFPath.gte_base, batch_size=100, chunk_size=8192, num_pass=2, use_random_chunk_size=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
